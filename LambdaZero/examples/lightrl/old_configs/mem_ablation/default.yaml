main:
  seed: 0 # type=int, default=0 - Leave 0 - seed will be chosen depending on run_id
  use_gpu: true

  plot: True # log & plot into Wandb
  stats_window_size: 4096 # Window for plotting stats about Training R
  log_topk_interval: 50  # Training step freq to log top k -> procs * num_steps * interval frames
  log_topk: 100  # Training step freq to log top k -> procs * num_steps * interval frames

#  save_interval: 10 # Number of training steps between two saves
#  save_best: False # Save separately best model
#  save_all: False # Save separate model for each save_interval
#  frames: 2.e+6 # Number of frames of training (procs number of frames / step are processed)

env_cfg:  # Example of envs cfg -
  env: BlockMolEnvGraph-v1 # Env name
  procs: 16 # Number of parallel environments to run
  actual_procs: 8 # Actual number of CPU threads to distribute the environments
  wrapper: ["ProxyCandidateRewardWithScoreMem"]  #"ProxyCandidateReward"]  #  List of wrappers - applied in reverse order
  precalculated_scores: True

  env_args:
    reward: "DummyReward"
    random_steps: 3
    max_steps: 7
    allow_removal: True
    env_seed: 3
    pre_saved_graph: True

eval_env_cfg:  # Example of envs cfg -
  env: BlockMolEnvGraph-v1 # Env name
  procs: 0 # Number of parallel environments to run
  actual_procs: 1 # Actual number of CPU threads to distribute the environments
  eval_mode: True
  env_args:
    env_seed: [13]
    eval_mode: True

proxy_dock: /scratch/andrein/lz2/results/2021Jun05-125747_breg/0001_dataset.weighted_sampler_False/0/best_model.pk
algo: ppo
log_interval: 1
save_interval: 10000
eval_interval: 0
eval_episodes: 32
eval_eps: 0.05
repeat_eval_eps: 1
eval_determinitistic: False
log_monitor: False

num_env_steps: 30.0e+7 # 30.0e+7  # 81920  # 30.0e+7
offset: 0
name: ""
cuda_deterministic: False

num_steps: 256
ppo_epoch: 4
num_mini_batch: 32

use_linear_lr_decay: true
lr: 2.5e-4
eps: 1.0e-05
alpha: 0.99
gamma: 0.99
use_gae: True
gae_lambda: 0.95
entropy_coef: 0.05
value_loss_coef: 0.5
max_grad_norm: 0.5
clip_param: 0.01

policy:
  sample_best: False

model:
  name: MPNNcond # MPNNcond # DeeperGCNSteps # MPNNcond # MPNNextra # EGNNetRLBO #MPNNet_v2  # MPNNetMultihead
  recurrent: False
  dim: 128
  use_init: True
  num_out: 2
  levels: 6


#  conditioning: [["r_steps", 10], ["rcond", 1]]
#  cond_level: 24
#  dropout: 0

#  dropout: 0
##  levels: 6
#  dim: 256
#  m_dim: 256
#

gail: False
gail_batch_size: 128
gail_epoch: 5
use_proper_time_limits: False
