from torch_geometric.data import Data, InMemoryDataset, download_url
from recover.utils import get_project_root
# import pubchempy as pcp
# from cmapPy.pandasGEXpress.parse import parse
import pickle
import pandas as pd
import torch
import os
import gzip
import shutil
from tqdm import tqdm


class L1000(InMemoryDataset):
    def __init__(self, transform=None, pre_transform=None):
        """
        Dataset object for the LINCS L1000 dataset.
        """

        super().__init__(os.path.join(get_project_root(), 'LINCS/'), transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

        print("L1000 dataset loaded.")

    @property
    def raw_file_names(self):
        """ The first 5 files correspond to phase 2, the last 3 to phase 1"""
        return [
            'Level5_COMPZ_n118050x12328_2017-03-06.gctx',
            'sig_info_2017-03-06.txt',
            'gene_info_2017-03-06.txt',
            'pert_info_2017-03-06.txt',
            'cell_info_2017-04-28.txt',
            'GSE92742_Broad_LINCS_Level5_COMPZ.MODZ_n473647x12328.gctx',
            'GSE92742_Broad_LINCS_sig_info.txt',
            'GSE92742_Broad_LINCS_pert_info.txt'
        ]

    @property
    def processed_file_names(self):
        return ['l1000_data.pt']

    def download(self):
        urls = [
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE70138&format=file&file=GSE70138%5FBroad%5FLINCS%5FLevel5'
            '%5FCOMPZ%5Fn118050x12328%5F2017%2D03%2D06%2Egctx%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE70138&format=file&file=GSE70138%5FBroad%5FLINCS%5Fsig%5F'
            'info%5F2017%2D03%2D06%2Etxt%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE70138&format=file&file=GSE70138%5FBroad%5FLINCS%5Fgene%5'
            'Finfo%5F2017%2D03%2D06%2Etxt%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE70138&format=file&file=GSE70138%5FBroad%5FLINCS%5Fpert%5'
            'Finfo%5F2017%2D03%2D06%2Etxt%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE70138&format=file&file=GSE70138%5FBroad%5FLINCS%5Fcell%5'
            'Finfo%5F2017%2D04%2D28%2Etxt%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE92742&format=file&file=GSE92742%5FBroad%5FLINCS%5FLevel5'
            '%5FCOMPZ%2EMODZ%5Fn473647x12328%2Egctx%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE92742&format=file&file=GSE92742%5FBroad%5FLINCS%5Fsig%5F'
            'info%2Etxt%2Egz',
            'https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE92742&format=file&file=GSE92742%5FBroad%5FLINCS%5Fpert%5'
            'Finfo%2Etxt%2Egz'
        ]

        for i in range(len(urls)):
            download_url(urls[i], self.raw_dir)
            with gzip.open(os.path.join(self.raw_dir, urls[i].split('/')[-1]), 'rb') as f_in:
                with open(os.path.join(self.raw_dir, self.raw_file_names[i]), 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)

    def process(self):
        # Load metadata
        self.sig_info, self.landmark_gene_list = self.load_metadata()
        self.expr_data = pd.concat([self.load_expr_data("phase1"), self.load_expr_data("phase2")], sort=False)

        data_df = pd.concat((self.expr_data, self.sig_info), axis=1)  # Concat with metadata

        # Extract numpy arrays
        gene_expr = data_df[data_df.columns[:len(self.landmark_gene_list)]].to_numpy()
        cid = data_df['cid'].to_numpy()
        cell_id = data_df['cell_id'].to_numpy()
        dose = data_df['pert_idose_value'].to_numpy()
        incub_time = data_df['pert_itime_value'].to_numpy()

        # Create data object
        data = Data(gene_expr=torch.tensor(gene_expr, dtype=torch.float),
                    cid=torch.tensor(cid, dtype=torch.long),
                    cell_id=cell_id,
                    dose=torch.tensor(dose, dtype=torch.float),
                    incub_time=torch.tensor(incub_time, dtype=torch.float))

        data_list = [data]
        if self.pre_transform is not None:
            data_list = self.pre_transform(data_list)

        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

    def load_metadata(self):
        # Get list of landmark genes. Gene info and cell info are the same for both phases
        gene_info = pd.read_csv(os.path.join(self.raw_dir, self.raw_file_names[2]), sep="\t")
        landmark_gene_list = gene_info[gene_info['pr_is_lm'] == 1]["pr_gene_id"].astype(str)

        # Load pert_info
        pert_info_with_cid_path = os.path.join(self.raw_dir, "pert_info_with_cid.txt")
        if os.path.isfile(pert_info_with_cid_path):
            pert_info = pd.read_csv(pert_info_with_cid_path, index_col="pert_id")
        else:
            print("Retrieving cids from PubChem, only happens the first time...")
            # Load both phases
            pert_info_1 = pd.read_csv(os.path.join(self.raw_dir, self.raw_file_names[3]), sep="\t",
                                    index_col="pert_id", usecols=["pert_id", "canonical_smiles"])
            pert_info_2 = pd.read_csv(os.path.join(self.raw_dir, self.raw_file_names[7]), sep="\t",
                                      index_col="pert_id", usecols=["pert_id", "canonical_smiles"])
            pert_info = pd.concat([pert_info_1, pert_info_2])
            # Remove duplicate indices
            pert_info = pert_info.loc[~pert_info.index.duplicated(keep='first')]

            pert_info = pert_info[pert_info['canonical_smiles'] != '-666']
            pert_info['cid'] = - 1
            for i in tqdm(pert_info.index):
                try:
                    pert_info.at[i, 'cid'] = pcp.get_compounds(pert_info.at[i, "canonical_smiles"], 'smiles')[0].cid
                except:
                    pass
            pert_info.to_csv(pert_info_with_cid_path)

        # Load sig_info for both phases
        sig_info_1 = pd.read_csv(os.path.join(self.raw_dir, self.raw_file_names[1]), sep="\t",
                               index_col="sig_id",
                               usecols=["sig_id", "pert_id", "cell_id", "pert_idose", "pert_itime"])
        sig_info_2 = pd.read_csv(os.path.join(self.raw_dir, self.raw_file_names[6]), sep="\t",
                                 index_col="sig_id",
                                 usecols=["sig_id", "pert_id", "cell_id", "pert_idose", "pert_itime"])
        sig_info = pd.concat([sig_info_1, sig_info_2])

        # Convert time to float and add to sig_info
        sig_info['pert_itime_value'] = sig_info['pert_itime'].apply(self.get_time)
        # Convert concentrations to float and add to sig_info
        sig_info['pert_idose_value'] = sig_info['pert_idose'].apply(self.get_concentration)

        # Add cid to sig_info
        pert_to_cid_dict = pert_info['cid'].to_dict()
        sig_info['cid'] = sig_info['pert_id'].apply(
            lambda s: pert_to_cid_dict[s] if s in pert_to_cid_dict.keys() else -1)
        sig_info = sig_info[sig_info['cid'] != -1]

        return sig_info, landmark_gene_list

    def load_expr_data(self, phase):
        assert phase in ["phase1", "phase2"]
        if phase == "phase1":
            df_path = os.path.join(self.raw_dir, "dataframe_phase1.pkl")
            file_name = self.raw_file_names[5]
        else:
            df_path = os.path.join(self.raw_dir, "dataframe_phase2.pkl")
            file_name = self.raw_file_names[0]
        if os.path.isfile(df_path):
            pickle_in = open(df_path, "rb")
            expr_data = pickle.load(pickle_in)
        else:  # If the data has not been saved yet, parse the original file and save dataframe
            print("Parsing original data, only happens the first time...")
            expr_data = parse(os.path.join(self.raw_dir, file_name), rid=self.landmark_gene_list).data_df.T
            # Ensure that the order of columns corresponds to landmark_gene_list
            expr_data = expr_data[self.landmark_gene_list]
            # Remove rows that are not in sig_info
            expr_data = expr_data[expr_data.index.isin(self.sig_info.index)]

            # Save data
            pickle_out = open(df_path, "wb")
            pickle.dump(expr_data, pickle_out, protocol=2)
            pickle_out.close()
        return expr_data

    def get_concentration(self, s):
        if s.endswith('ÂµM') or s.endswith('um'):
            return float(s[:-3])
        if s.endswith('nM'):
            return 0.001 * float(s[:-3])
        return -1

    def get_time(self, s):
        return float(s[:-2])


if __name__ == '__main__':
    dataset = L1000()
    print("toto")

